{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_train_dict_torch.pkl', 'rb') as f:\n",
    "    data_train = pickle.load(f)\n",
    "\n",
    "with open('data_test_dict_torch.pkl', 'rb') as f:\n",
    "    data_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_channels = []\n",
    "max_channels = []\n",
    "train_dims = [100, 120, 140, 160, 180]\n",
    "for we, sn in np.random.permutation([*product(train_dims, train_dims)]):\n",
    "    min_channels.append(data_train[f'{we}x{sn}'].transpose(0, 1).reshape(15, -1).min(dim=1)[0])\n",
    "    max_channels.append(data_train[f'{we}x{sn}'].transpose(0, 1).reshape(15, -1).max(dim=1)[0])\n",
    "min_scale = torch.stack(min_channels).min(dim=0)[0]\n",
    "max_scale = torch.stack(max_channels).max(dim=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_batch(batch, min_scale, max_scale):\n",
    "    min = min_scale[None,...,None, None].to(batch.device)\n",
    "    max = max_scale[None,...,None, None].to(batch.device)\n",
    "    return (batch - min) / (max - min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_minibatches(*tensors, batch_size, shuffle=True, epochs=1,\n",
    "                        allow_incomplete=True, callback=lambda x:x):\n",
    "    indices = np.arange(len(tensors[0]))\n",
    "    upper_bound = int((np.ceil if allow_incomplete else np.floor) (len(indices) / batch_size)) * batch_size\n",
    "    epoch = 0\n",
    "    while True:\n",
    "        if shuffle:\n",
    "            np.random.shuffle(indices)\n",
    "        for batch_start in callback(range(0, upper_bound, batch_size)):\n",
    "            batch_ix = indices[batch_start: batch_start + batch_size]\n",
    "            batch = [tensor[batch_ix] for tensor in tensors]\n",
    "            yield batch if len(tensors) > 1 else batch[0]\n",
    "        epoch += 1\n",
    "        if epoch >= epochs:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bayesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_channels=15, \n",
    "                    conv_kernel_0 = 3, adaptive_size = 90, \n",
    "                    conv_kernel_1 = 3, pool_kernel_1 = 3, pool_stride_1 = 2, \n",
    "                    conv_kernel_2 = 3, pool_kernel_2 = 3, pool_stride_2 = 2,\n",
    "                    conv_kernel_3 = 3, pool_kernel_3 = 3, pool_stride_3 = 2,\n",
    "                    upsample_1 = 16,\n",
    "                    upsample_2 = 24,\n",
    "                    upsample_3 = 52,\n",
    "                    first_layer = 10, second_layer = 6, third_layer = 1):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_channels, out_channels=15, kernel_size=conv_kernel_0),\n",
    "            nn.BatchNorm2d(15),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d(output_size=(adaptive_size, adaptive_size)), \n",
    "            \n",
    "            nn.Conv2d(in_channels=15, out_channels=first_layer, kernel_size=conv_kernel_1),\n",
    "            nn.BatchNorm2d(first_layer),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=pool_kernel_1, stride=pool_stride_1), # a = math.ceil((adaptive_size - (conv_kernel_1 - 1)) / pool_stride_1) - 1\n",
    "\n",
    "            nn.Conv2d(in_channels=first_layer, out_channels=second_layer, kernel_size=conv_kernel_2),\n",
    "            nn.BatchNorm2d(second_layer),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=pool_kernel_2, stride=pool_stride_2), # b = math.ceil((a - (conv_kernel_2 - 1)) / pool_stride_2) - 1\n",
    "\n",
    "            nn.Conv2d(in_channels=second_layer, out_channels=third_layer, kernel_size=conv_kernel_3),\n",
    "            nn.BatchNorm2d(third_layer),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=pool_kernel_3, stride=pool_stride_3), # c = math.ceil((b - (conv_kernel_3 - 1)) / pool_stride_3) - 1\n",
    "\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "        self.encoder = self.encoder.float() # c = math.ceil((math.ceil((math.ceil((adaptive_size - (conv_kernel_1 - 1)) / pool_stride_1) - 1 - (conv_kernel_2 - 1)) / pool_stride_2) - 1 - (conv_kernel_3 - 1)) / pool_stride_3) - 1\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Unflatten(1, (third_layer, int(math.ceil((math.ceil((math.ceil((adaptive_size - (conv_kernel_1 - 1)) / pool_stride_1) - 1 - (conv_kernel_2 - 1)) / pool_stride_2) - 1 - (conv_kernel_3 - 1)) / pool_stride_3) - 1), \n",
    "                            int(math.ceil((math.ceil((math.ceil((adaptive_size - (conv_kernel_1 - 1)) / pool_stride_1) - 1 - (conv_kernel_2 - 1)) / pool_stride_2) - 1 - (conv_kernel_3 - 1)) / pool_stride_3) - 1))),\n",
    "            \n",
    "            nn.ConvTranspose2d(in_channels=third_layer, out_channels=second_layer, kernel_size=conv_kernel_3),\n",
    "            nn.Upsample(size=(upsample_1, upsample_1)),\n",
    "            nn.ConvTranspose2d(in_channels=second_layer, out_channels=first_layer, kernel_size=conv_kernel_2),\n",
    "            nn.Upsample(size=(upsample_2, upsample_2)),\n",
    "            nn.ConvTranspose2d(in_channels=first_layer, out_channels=15, kernel_size=conv_kernel_1),\n",
    "            nn.Upsample(size=(upsample_3, upsample_3)),\n",
    "            nn.ConvTranspose2d(in_channels=15, out_channels=15, kernel_size=conv_kernel_0),\n",
    "        )\n",
    "        self.decoder = self.decoder.float()\n",
    "\n",
    "    def forward(self, features):\n",
    "        emb = self.encoder(features)\n",
    "        reconstructed = self.decoder(emb)\n",
    "    \n",
    "        _, _, w, h = features.shape\n",
    "        reconstructed = nn.Upsample(size=(w, h))(reconstructed)\n",
    "    \n",
    "        if reconstructed.shape != features.shape:\n",
    "            print(reconstructed.shape)\n",
    "            print(features.shape)\n",
    "            assert False\n",
    "    \n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    device = 'cuda'\n",
    "    print(params)\n",
    "\n",
    "    model = Autoencoder(first_layer = int(params['first_layer']),\n",
    "                        second_layer = int(params['second_layer']),\n",
    "                        third_layer = int(params['third_layer']),\n",
    "                        adaptive_size = int(params['adaptive_size']), \n",
    "                        upsample_1 = int(params['upsample_1']),\n",
    "                        upsample_2 = int(params['upsample_2']),\n",
    "                        upsample_3 = int(params['upsample_3'])).to(device)\n",
    "\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.MSELoss()\n",
    "    test_loss = defaultdict(list)\n",
    "    \n",
    "    for epoch in range(25):\n",
    "        for we, sn in np.random.permutation([*product(train_dims, train_dims)]):\n",
    "            for batch in iterate_minibatches(data_train[f'{we}x{sn}'], batch_size=40):\n",
    "                opt.zero_grad()\n",
    "                batch = scale_batch(batch.to(device), min_scale, max_scale)\n",
    "                out = model(batch)\n",
    "                loss = criterion(out, batch)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "        for we, sn in [*product(train_dims, train_dims)]:\n",
    "            test_batch = scale_batch(data_test[f'{we}x{sn}'].to(device), min_scale, max_scale)\n",
    "            out = model(test_batch)\n",
    "            test_loss[f'{we}x{sn}'].append(criterion(out, test_batch).item())\n",
    "        mse_test = np.array([np.array(test_loss[f'{we}x{sn}']) for we, sn in [*product(train_dims, train_dims)]]).mean(0)\n",
    "        # return mse_test[-1]\n",
    "    return {'loss': mse_test[-1], 'params': params, 'status': STATUS_OK}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adaptive_size': 100, 'first_layer': 13, 'second_layer': 7, 'third_layer': 3, 'upsample_1': 12, 'upsample_2': 40, 'upsample_3': 56}\n",
      "  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [03:11<00:00, 191.05s/trial, best loss: 0.024075031057000162]\n",
      "{'adaptive_size': 4, 'first_layer': 3, 'second_layer': 1, 'third_layer': 1, 'upsample_1': 1, 'upsample_2': 3, 'upsample_3': 3}\n",
      "{'adaptive_size': 70, 'first_layer': 13, 'second_layer': 6, 'third_layer': 2, 'upsample_1': 16, 'upsample_2': 36, 'upsample_3': 48}\n",
      "100%|██████████| 2/2 [03:31<00:00, 211.92s/trial, best loss: 0.024075031057000162]\n",
      "{'adaptive_size': 4, 'first_layer': 3, 'second_layer': 1, 'third_layer': 1, 'upsample_1': 1, 'upsample_2': 3, 'upsample_3': 3}\n",
      "{'adaptive_size': 90, 'first_layer': 11, 'second_layer': 9, 'third_layer': 3, 'upsample_1': 12, 'upsample_2': 36, 'upsample_3': 60}\n",
      "100%|██████████| 3/3 [03:47<00:00, 227.98s/trial, best loss: 0.024075031057000162]\n",
      "{'adaptive_size': 4, 'first_layer': 3, 'second_layer': 1, 'third_layer': 1, 'upsample_1': 1, 'upsample_2': 3, 'upsample_3': 3}\n",
      "{'adaptive_size': 60, 'first_layer': 11, 'second_layer': 9, 'third_layer': 2, 'upsample_1': 16, 'upsample_2': 32, 'upsample_3': 44}\n",
      "100%|██████████| 4/4 [03:35<00:00, 215.59s/trial, best loss: 0.024075031057000162]\n",
      "{'adaptive_size': 4, 'first_layer': 3, 'second_layer': 1, 'third_layer': 1, 'upsample_1': 1, 'upsample_2': 3, 'upsample_3': 3}\n",
      "{'adaptive_size': 100, 'first_layer': 12, 'second_layer': 6, 'third_layer': 5, 'upsample_1': 12, 'upsample_2': 28, 'upsample_3': 48}\n",
      "100%|██████████| 5/5 [03:52<00:00, 232.03s/trial, best loss: 0.024075031057000162]\n",
      "{'adaptive_size': 4, 'first_layer': 3, 'second_layer': 1, 'third_layer': 1, 'upsample_1': 1, 'upsample_2': 3, 'upsample_3': 3}\n",
      "{'adaptive_size': 70, 'first_layer': 10, 'second_layer': 6, 'third_layer': 5, 'upsample_1': 24, 'upsample_2': 32, 'upsample_3': 52}\n",
      "100%|██████████| 6/6 [03:42<00:00, 222.49s/trial, best loss: 0.02089968614280224]\n",
      "{'adaptive_size': 1, 'first_layer': 0, 'second_layer': 0, 'third_layer': 3, 'upsample_1': 3, 'upsample_2': 1, 'upsample_3': 2}\n",
      "{'adaptive_size': 70, 'first_layer': 11, 'second_layer': 8, 'third_layer': 4, 'upsample_1': 8, 'upsample_2': 36, 'upsample_3': 52}\n",
      "100%|██████████| 7/7 [03:47<00:00, 227.69s/trial, best loss: 0.02089968614280224]\n",
      "{'adaptive_size': 1, 'first_layer': 0, 'second_layer': 0, 'third_layer': 3, 'upsample_1': 3, 'upsample_2': 1, 'upsample_3': 2}\n",
      "{'adaptive_size': 60, 'first_layer': 13, 'second_layer': 7, 'third_layer': 4, 'upsample_1': 8, 'upsample_2': 28, 'upsample_3': 44}\n",
      "100%|██████████| 8/8 [03:37<00:00, 217.37s/trial, best loss: 0.02089968614280224]\n",
      "{'adaptive_size': 1, 'first_layer': 0, 'second_layer': 0, 'third_layer': 3, 'upsample_1': 3, 'upsample_2': 1, 'upsample_3': 2}\n",
      "{'adaptive_size': 90, 'first_layer': 12, 'second_layer': 6, 'third_layer': 4, 'upsample_1': 16, 'upsample_2': 36, 'upsample_3': 48}\n",
      "100%|██████████| 9/9 [03:44<00:00, 224.59s/trial, best loss: 0.02089968614280224]\n",
      "{'adaptive_size': 1, 'first_layer': 0, 'second_layer': 0, 'third_layer': 3, 'upsample_1': 3, 'upsample_2': 1, 'upsample_3': 2}\n",
      "{'adaptive_size': 70, 'first_layer': 11, 'second_layer': 9, 'third_layer': 5, 'upsample_1': 16, 'upsample_2': 36, 'upsample_3': 44}\n",
      "100%|██████████| 10/10 [03:44<00:00, 224.46s/trial, best loss: 0.02089968614280224]\n",
      "{'adaptive_size': 1, 'first_layer': 0, 'second_layer': 0, 'third_layer': 3, 'upsample_1': 3, 'upsample_2': 1, 'upsample_3': 2}\n"
     ]
    }
   ],
   "source": [
    "# possible values of parameters\n",
    "space={'first_layer' : hp.choice('first_layer', [10, 11, 12, 13]), \n",
    "      'second_layer' : hp.choice('second_layer', [6, 7, 8, 9]), \n",
    "      'third_layer' : hp.choice('third_layer', [2, 3, 4, 5]), \n",
    "      'adaptive_size' : hp.choice('adaptive_size', [60, 70, 80, 90, 100]), \n",
    "      'upsample_1' : hp.choice('upsample_1', [8, 12, 16, 24]),\n",
    "      'upsample_2' : hp.choice('upsample_2', [28, 32, 36, 40]),\n",
    "      'upsample_3' : hp.choice('upsample_3', [44, 48, 52, 56, 60])\n",
    "      }\n",
    "\n",
    "trials = Trials()\n",
    "\n",
    "max_evals = 10\n",
    "for i in range(1, max_evals + 1, 1):\n",
    "    best=fmin(fn=objective,\n",
    "          space=space, \n",
    "          algo=tpe.suggest,\n",
    "          max_evals=i,\n",
    "          trials=trials,\n",
    "          show_progressbar=True\n",
    "         )\n",
    "\n",
    "    print(best)\n",
    "   \n",
    "    pickle.dump(trials, open(\"bayesian_channels_results.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter_done = len(trials.results)\n",
    "max_evals = 90\n",
    "iter_done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1 + iter_done, max_evals + 1 + iter_done, 1):\n",
    "    best=fmin(fn=objective,\n",
    "          space=space, \n",
    "          algo=tpe.suggest,\n",
    "          max_evals=i,\n",
    "          trials=trials, \n",
    "          show_progressbar=True\n",
    "         )\n",
    "    print(best)\n",
    "\n",
    "    # pickle.dump(trials, open(\"bayesian_channels_results.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open('trials.pkl', 'wb') as f:\n",
    "#    pickle.dump(trials, f)\n",
    "\n",
    "# with open('best_params.pkl', 'wb') as f:\n",
    "#    pickle.dump(best, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузка результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = []\n",
    "with (open('bayesian_channels_results.pkl','rb')) as openfile:\n",
    "    while True:\n",
    "        try:\n",
    "            file.append(pickle.load(openfile))\n",
    "        except EOFError:\n",
    "            break\n",
    "df = pd.DataFrame(file[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
